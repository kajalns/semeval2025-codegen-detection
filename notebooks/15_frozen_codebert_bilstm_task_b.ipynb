{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMdT99s/SSbNUMErxA8oKmY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IeDgYd_4oHAe"},"outputs":[],"source":["!pip -q install transformers datasets accelerate\n","import os\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","\n","from transformers import AutoTokenizer, AutoModel\n","\n","from google.colab import drive, files\n","\n","\n","drive.mount(\"/content/drive\", force_remount=False)\n","CODEBERT_DIR = \"/content/drive/MyDrive/semeval_task13_models/task_b/codebert_v1\"\n"]},{"cell_type":"code","source":["ROOT = Path(\"/content/semeval_task13\")\n","if (ROOT / \"SemEval-2026-Task13\").exists():\n","    BASE_DIR = ROOT / \"SemEval-2026-Task13\"\n","else:\n","    BASE_DIR = ROOT\n","\n","TASK_B_DIR = BASE_DIR / \"task_b\"\n","print(\"TASK_B_DIR:\", TASK_B_DIR)\n","\n","train_path = TASK_B_DIR / \"task_b_training_set.parquet\"\n","val_path   = TASK_B_DIR / \"task_b_validation_set.parquet\"\n","test_path  = \"test.parquet\"\n","\n","df_train = pd.read_parquet(train_path)\n","df_val   = pd.read_parquet(val_path)\n","df_test  = pd.read_parquet(test_path)\n","\n","print(\"Train:\", df_train.shape)\n","print(\"Val  :\", df_val.shape)\n","print(\"Test :\", df_test.shape)\n","print(\"Train columns:\", df_train.columns.tolist())\n","\n","#Labels 0..10 for Subtask B\n","df_train = df_train.copy()\n","df_val   = df_val.copy()\n","df_test  = df_test.copy()\n","\n","df_train.rename(columns={\"label\": \"labels\"}, inplace=True)\n","df_val.rename(columns={\"label\": \"labels\"}, inplace=True)\n","\n","y_train = df_train[\"labels\"].astype(int).values\n","y_val   = df_val[\"labels\"].astype(int).values\n","\n","\n","#Tokenizer & HF Datasets from the fine-tuned CodeBERT dir\n","tokenizer = AutoTokenizer.from_pretrained(CODEBERT_DIR, use_fast=True)\n","MAX_LENGTH = 256\n","\n","def make_hf_datasets(df_train_local, df_val_local, df_test_local, max_length=256):\n","    def tokenize_batch(batch):\n","        return tokenizer(\n","            batch[\"code\"],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=max_length,\n","        )\n","\n","    train_ds = Dataset.from_pandas(df_train_local[[\"code\", \"labels\"]])\n","    val_ds   = Dataset.from_pandas(df_val_local[[\"code\", \"labels\"]])\n","    test_ds  = Dataset.from_pandas(df_test_local[[\"code\"]])\n","\n","    train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"code\"])\n","    val_tok   = val_ds.map(tokenize_batch,   batched=True, remove_columns=[\"code\"])\n","    test_tok  = test_ds.map(tokenize_batch,  batched=True, remove_columns=[\"code\"])\n","\n","    train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","    val_tok.set_format(type=\"torch\",   columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","    test_tok.set_format(type=\"torch\",  columns=[\"input_ids\", \"attention_mask\"])\n","\n","    return train_tok, val_tok, test_tok\n","\n","train_tok, val_tok, test_tok = make_hf_datasets(df_train, df_val, df_test, max_length=MAX_LENGTH)\n","print(\"Tokenized train size:\", len(train_tok))\n","print(\"Tokenized val size  :\", len(val_tok))\n","print(\"Tokenized test size :\", len(test_tok))\n","\n","\n","#Model: Fine-tuned CodeBERT encoder (frozen) + BiLSTM + Linear\n","class CodeBertBiLSTMClassifier(nn.Module):\n","    def __init__(self, load_dir, num_labels, lstm_hidden_size=256, freeze_codebert=True):\n","        super().__init__()\n","        #load encoder weights from the fine-tuned CodeBERT dir\n","        self.codebert = AutoModel.from_pretrained(load_dir)\n","        hidden_size = self.codebert.config.hidden_size\n","\n","        if freeze_codebert:\n","            for p in self.codebert.parameters():\n","                p.requires_grad = False\n","\n","        self.bilstm = nn.LSTM(\n","            input_size=hidden_size,\n","            hidden_size=lstm_hidden_size,\n","            num_layers=1,\n","            batch_first=True,\n","            bidirectional=True,\n","        )\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(2 * lstm_hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        #CodeBERT encoder without classification head\n","        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_state = outputs.last_hidden_state #(B, L, H)\n","\n","        #BiLSTM\n","        lstm_out, (h_n, c_n) = self.bilstm(last_hidden_state)\n","        # h_n: (num_directions=2, B, hidden)\n","        h_forward  = h_n[0]  #(B, hidden)\n","        h_backward = h_n[1]  #(B, hidden)\n","        h_cat = torch.cat([h_forward, h_backward], dim=-1) #(B, 2*hidden)\n","\n","        x = self.dropout(h_cat)\n","        logits = self.classifier(x) #(B, num_labels)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits, labels)\n","\n","        return logits, loss\n","\n","#Dataloading\n","batch_size = 8\n","\n","train_loader = DataLoader(train_tok, batch_size=batch_size, shuffle=True)\n","val_loader   = DataLoader(val_tok,   batch_size=batch_size, shuffle=False)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","num_labels = 11\n","\n","model = CodeBertBiLSTMClassifier(\n","    load_dir=CODEBERT_DIR,\n","    num_labels=num_labels,\n","    lstm_hidden_size=256,\n","    freeze_codebert=True,\n",")\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(\n","    [p for p in model.parameters() if p.requires_grad],\n","    lr=2e-4,\n",")\n","\n","num_epochs = 2\n","\n","#evaluation function\n","def eval_on_val():\n","    model.eval()\n","    all_logits = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attn      = batch[\"attention_mask\"].to(device)\n","            labels    = batch[\"labels\"].to(device)\n","\n","            logits, loss = model(input_ids=input_ids, attention_mask=attn, labels=labels)\n","            all_logits.append(logits.cpu().numpy())\n","            all_labels.append(labels.cpu().numpy())\n","\n","    all_logits = np.concatenate(all_logits, axis=0)\n","    all_labels = np.concatenate(all_labels, axis=0)\n","    preds = all_logits.argmax(axis=1)\n","\n","    acc = accuracy_score(all_labels, preds)\n","    f1  = f1_score(all_labels, preds, average=\"macro\")\n","\n","    print(f\"VAL Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")\n","    print(\"\\nClassification report (val):\")\n","    print(classification_report(all_labels, preds, digits=3))\n","\n","    return acc, f1\n","\n","#training loop\n","for epoch in range(1, num_epochs + 1):\n","    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n","    model.train()\n","    total_loss = 0.0\n","\n","    for step, batch in enumerate(train_loader):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attn      = batch[\"attention_mask\"].to(device)\n","        labels    = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad()\n","        logits, loss = model(input_ids=input_ids, attention_mask=attn, labels=labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        if (step + 1) % 200 == 0:\n","            avg = total_loss / (step + 1)\n","            print(f\"  step {step+1}, train loss: {avg:.4f}\")\n","\n","    avg_epoch_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch} finished, avg train loss: {avg_epoch_loss:.4f}\")\n","    eval_on_val()\n","\n","\n"],"metadata":{"id":"vuDd3kkwsEz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#test prediction\n","test_loader = DataLoader(test_tok, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_test_logits = []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attn      = batch[\"attention_mask\"].to(device)\n","\n","        logits, _ = model(input_ids=input_ids, attention_mask=attn, labels=None)\n","        all_test_logits.append(logits.cpu().numpy())\n","\n","all_test_logits = np.concatenate(all_test_logits, axis=0)\n","test_preds = all_test_logits.argmax(axis=1).astype(int)\n","\n","print(\"Num test preds:\", len(test_preds))\n","print(\"First 10 labels:\", test_preds[:10])\n","\n","print(\"\\nupload Task B sample_submission.csv\")\n","\n","\n","sample_filename = \"sample_submission_b.csv\"\n","sample_sub = pd.read_csv(sample_filename)\n","print(\"Sample submission shape:\", sample_sub.shape)\n","print(sample_sub.head())\n","\n","if len(sample_sub) != len(test_preds):\n","    print(\"Length mismatch:\", len(sample_sub), \"vs\", len(test_preds))\n","else:\n","    if \"label\" in sample_sub.columns:\n","        label_col = \"label\"\n","    else:\n","        label_col = sample_sub.columns[1]\n","\n","    sample_sub[label_col] = test_preds\n","    print(\"\\nSubmission preview (CodeBERT+BiLSTM):\")\n","    print(sample_sub.head())\n","\n","    out_name = \"subtask_b_codebert_bilstm.csv\"\n","    sample_sub.to_csv(out_name, index=False)\n","    print(f\"\\nsaved submission file: {out_name}\")"],"metadata":{"id":"gPQjucwdtrZt"},"execution_count":null,"outputs":[]}]}