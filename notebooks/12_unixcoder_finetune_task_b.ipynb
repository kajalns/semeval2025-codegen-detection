{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6IsD7wADFP2v"},"outputs":[],"source":["!pip -q install transformers datasets accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lvM2mzO5Ena"},"outputs":[],"source":["import numpy as np\n","from datasets import Dataset\n","from pathlib import Path\n","import os\n","import pandas as pd\n","import torch\n","from torch.nn.functional import softmax\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from transformers import(\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_nsdPpK54T-"},"outputs":[],"source":["ROOT = Path(\"/content/semeval_task13\")\n","\n","if (ROOT / \"SemEval-2026-Task13\").exists():\n","    BASE_DIR = ROOT / \"SemEval-2026-Task13\"\n","else:\n","    BASE_DIR = ROOT\n","\n","TASK_DIR = BASE_DIR / \"task_b\"\n","\n","print(\"BASE_DIR:\", BASE_DIR)\n","print(\"TASK_DIR:\", TASK_DIR)\n","print(\"Files in task_b:\")\n","for p in TASK_DIR.iterdir():\n","    print(\" -\", p.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j--QV9Vc58wG"},"outputs":[],"source":["\n","train_path = TASK_DIR / \"task_b_training_set.parquet\"\n","val_path  = TASK_DIR / \"task_b_validation_set.parquet\"\n","test_path = TASK_DIR / \"task_b_test_set_sample.parquet\"\n","\n","df_train = pd.read_parquet(train_path)\n","df_val   = pd.read_parquet(val_path)\n","df_test  = pd.read_parquet(test_path)\n","\n","print(\"Train shape:\", df_train.shape)\n","print(\"Val shape  :\", df_val.shape)\n","print(\"Test shape :\", df_test.shape)\n","print(\"\\nTrain columns:\", df_train.columns.tolist())\n","print(\"Val columns  :\", df_val.columns.tolist())\n","print(\"Test columns :\", df_test.columns.tolist())\n","\n","df_train.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egUWYr96yf-D"},"outputs":[],"source":["#label mapping\n","unique_labels = sorted(df_train[\"label\"].unique())\n","label2id_orig_to_idx = {lbl: i for i, lbl in enumerate(unique_labels)}\n","id2label_idx_to_orig = {i: lbl for lbl, i in label2id_orig_to_idx.items()}\n","num_labels = len(unique_labels)\n","\n","print(\"Number of labels:\", num_labels)\n","print(\"Original label -> index mapping:\", label2id_orig_to_idx)\n","\n","df_train = df_train.copy()\n","df_val   = df_val.copy()\n","df_test  = df_test.copy()\n","\n","df_train[\"labels\"] = df_train[\"label\"].map(label2id_orig_to_idx).astype(int)\n","df_val[\"labels\"]   = df_val[\"label\"].map(label2id_orig_to_idx).astype(int)\n","\n","\n","#tokenizer + HF Datasets\n","MODEL_NAME = \"microsoft/unixcoder-base\"\n","tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","\n","MAX_LENGTH = 256\n","\n","def make_hf_datasets(df_train_local, df_val_local, df_test_local, max_length=256):\n","    def tokenize_batch(batch):\n","        return tokenizer(\n","            batch[\"code\"],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=max_length,\n","        )\n","\n","    train_ds = Dataset.from_pandas(df_train_local[[\"code\", \"labels\"]])\n","    val_ds   = Dataset.from_pandas(df_val_local[[\"code\", \"labels\"]])\n","    test_ds  = Dataset.from_pandas(df_test_local[[\"code\"]])\n","\n","    train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"code\"])\n","    val_tok   = val_ds.map(tokenize_batch,   batched=True, remove_columns=[\"code\"])\n","    test_tok  = test_ds.map(tokenize_batch,  batched=True, remove_columns=[\"code\"])\n","\n","    train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","    val_tok.set_format(type=\"torch\",   columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","    test_tok.set_format(type=\"torch\",  columns=[\"input_ids\", \"attention_mask\"])\n","\n","    return train_tok, val_tok, test_tok\n","\n","train_tok, val_tok, test_tok = make_hf_datasets(df_train, df_val, df_test, max_length=MAX_LENGTH)\n","\n","#metric function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    acc = accuracy_score(labels, preds)\n","    f1  = f1_score(labels, preds, average=\"macro\")\n","    return {\"accuracy\": acc, \"macro_f1\": f1}\n","\n","#load the model\n","id2label_cfg = {i: str(id2label_idx_to_orig[i]) for i in id2label_idx_to_orig}\n","label2id_cfg = {str(v): k for k, v in id2label_idx_to_orig.items()}\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=num_labels,\n","    id2label=id2label_cfg,\n","    label2id=label2id_cfg,\n",")\n","\n","#TrainingArguments\n","batch_size = 8\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/task_b_unixcoder_runs\",   #temporary\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    logging_steps=200,\n","    save_steps=10_000_000,\n","    report_to=\"none\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tok,\n","    eval_dataset=val_tok,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#train and evaluation\n","trainer.train()\n","eval_results = trainer.evaluate(eval_dataset=val_tok)\n","print(\"UniXcoder Dev metrics:\", eval_results)\n","\n","dev_logits = trainer.predict(val_tok).predictions\n","dev_preds  = np.argmax(dev_logits, axis=-1)\n","y_true     = df_val[\"labels\"].values\n","\n","print(\"\\nClassification report (UniXcoder, index labels):\")\n","print(classification_report(y_true, dev_preds, digits=3))\n","\n","#test predictions\n","test_logits = trainer.predict(test_tok).predictions\n","test_preds_idx = np.argmax(test_logits, axis=-1).astype(int)\n","\n","print(\"Number of test predictions:\", len(test_preds_idx))\n","print(\"First 10 test_preds (indices):\", test_preds_idx[:10])\n","\n","# Map back to original label IDs using your mapping (for submission)\n","test_labels_mapped = [int(id2label_idx_to_orig[i]) for i in test_preds_idx]\n","print(\"First 10 mapped labels (original IDs):\", test_labels_mapped[:10])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOPoCYfZ5DVI"},"outputs":[],"source":["from google.colab import files\n","import pandas as pd\n","\n","print(\"upload the sample submission file\")\n","uploaded = files.upload()\n","\n","sample_filename = next(iter(uploaded.keys()))\n","print(\"Loaded file:\", sample_filename)\n","\n","sample_sub = pd.read_csv(sample_filename)\n","\n","print(\"Sample submission shape:\", sample_sub.shape)\n","print(\"First rows of sample submission:\")\n","print(sample_sub.head())\n","print(\"Number of test predictions:\", len(test_labels_mapped))\n","\n","# Safety check\n","if len(sample_sub) != len(test_labels_mapped):\n","    print(\"Length mismatch: sample_sub rows:\", len(sample_sub), \"| preds:\", len(test_labels_mapped))\n","else:\n","    label_col = \"label\"\n","    sample_sub[label_col] = test_labels_mapped\n","\n","    print(\"\\nSubmission preview:\")\n","    print(sample_sub.head())\n","\n","    sub_path = \"subtask_b_unixcoder.csv\"\n","    sample_sub.to_csv(sub_path, index=False)\n","    print(f\"\\nsaved submission file: {sub_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QvxB4HFALt_"},"outputs":[],"source":["#validation probablity(for ensemble)\n","dev_pred = trainer.predict(val_tok)\n","dev_logits = dev_pred.predictions\n","dev_probs  = softmax(torch.tensor(dev_logits), dim=-1).cpu().numpy()\n","\n","y_true = df_val[\"labels\"].values\n","dev_preds = dev_probs.argmax(axis=1)\n","\n","acc = accuracy_score(y_true, dev_preds)\n","f1  = f1_score(y_true, dev_preds, average=\"macro\")\n","print(\"Val Accuracy:\", acc)\n","print(\"Val Macro-F1:\", f1)\n","\n","#test probablity\n","test_pred = trainer.predict(test_tok)\n","test_logits = test_pred.predictions\n","test_probs  = softmax(torch.tensor(test_logits), dim=-1).cpu().numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfO5Iiu4ATbk"},"outputs":[],"source":["MODEL_KEY = \"unixcoder\"\n","\n","SAVE_ROOT = Path(\"/content/drive/MyDrive/semeval_task13_probs/task_b\")\n","SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","np.save(SAVE_ROOT / f\"{MODEL_KEY}_dev_probs.npy\",  dev_probs)\n","np.save(SAVE_ROOT / f\"{MODEL_KEY}_test_probs.npy\", test_probs)\n","\n","print(\"Saved probs for\", MODEL_KEY, \"to\", SAVE_ROOT)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP6GhcgTuEKOANUd44i1pGB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}