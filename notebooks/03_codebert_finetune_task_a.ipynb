{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyOYNGiRnqsfTEpfTULnR8cU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip -q install transformers accelerate datasets evaluate"],"metadata":{"id":"A8zrMTbAimoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"GiAimtzNRqIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5rFovDKUxYB"},"outputs":[],"source":["import os\n","import torch\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import joblib\n","from datasets import Dataset\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    confusion_matrix,\n","    classification_report,\n",")\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding,\n",")\n"]},{"cell_type":"code","source":["#data already downloaded using 01_data_overview_task_ab.ipynb\n","\n","ROOT = Path(\"/content/semeval_task13\")\n","\n","#handling extra folder layer if present\n","if (ROOT / \"SemEval-2026-Task13\").exists():\n","    BASE_DIR = ROOT / \"SemEval-2026-Task13\"\n","else:\n","    BASE_DIR = ROOT\n","\n","TASK_A_DIR = BASE_DIR / \"task_a\"\n","\n","print(\"BASE_DIR:\", BASE_DIR)\n","print(\"TASK_A_DIR:\", TASK_A_DIR)\n","print(\"Files in task_a:\")\n","for p in TASK_A_DIR.iterdir():\n","    print(\" -\", p.name)\n"],"metadata":{"id":"HdXvYC7eg3_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load subtask A data\n","\n","train_path = TASK_A_DIR / \"task_a_training_set_1.parquet\"\n","val_path   = TASK_A_DIR / \"task_a_validation_set.parquet\"\n","test_path  = TASK_A_DIR / \"task_a_test_set_sample.parquet\"\n","\n","df_train = pd.read_parquet(train_path)\n","df_val   = pd.read_parquet(val_path)\n","df_test  = pd.read_parquet(test_path)\n","\n","print(\"Train shape:\", df_train.shape)\n","print(\"Val shape  :\", df_val.shape)\n","print(\"Test shape :\", df_test.shape)\n","print(\"\\nColumns:\", df_train.columns.tolist())\n","\n","df_train.head()\n"],"metadata":{"id":"_h4zFyXPg7SK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#basic preprocessing\n","df_train = df_train[[\"code\", \"label\"]].dropna()\n","df_val   = df_val[[\"code\", \"label\"]].dropna()\n","\n","df_train[\"label\"] = df_train[\"label\"].astype(int)\n","df_val[\"label\"]   = df_val[\"label\"].astype(int)\n","\n","print(df_train.head())\n","print(df_val.head())\n","\n"],"metadata":{"id":"erRYVSTugxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","\n","train_ds = Dataset.from_pandas(df_train[[\"code\", \"label\"]], preserve_index=False)\n","val_ds   = Dataset.from_pandas(df_val[[\"code\", \"label\"]], preserve_index=False)\n","test_ds  = Dataset.from_pandas(df_test[[\"code\"]],          preserve_index=False)\n","\n","print(train_ds)\n","print(val_ds)\n","print(test_ds)\n"],"metadata":{"id":"3YakpaxTgxNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","model_name = \"microsoft/codebert-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","#pad token\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token or tokenizer.cls_token\n","\n","max_length = 256\n","\n","def tokenize_fn(batch):\n","    return tokenizer(\n","        batch[\"code\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=max_length,\n","    )\n","\n","train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"code\"])\n","val_tok   = val_ds.map(tokenize_fn,   batched=True, remove_columns=[\"code\"])\n","test_tok  = test_ds.map(tokenize_fn,  batched=True, remove_columns=[\"code\"])\n","\n","train_tok = train_tok.rename_column(\"label\", \"labels\")\n","val_tok   = val_tok.rename_column(\"label\", \"labels\")\n","\n","train_tok.set_format(type=\"torch\")\n","val_tok.set_format(type=\"torch\")\n","test_tok.set_format(type=\"torch\")\n"],"metadata":{"id":"1zq04Qm1gxQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n",")\n","import evaluate\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"WANDB_MODE\"] = \"disabled\"\n","\n","num_labels = 2\n","model_name = \"microsoft/codebert-base\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_labels,\n","    problem_type=\"single_label_classification\",\n",")\n","\n","#set pad token id\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","#macro F1 and accuracy\n","metric = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    macro_f1 = metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n","    acc = (preds == labels).mean()\n","    return {\"macro_f1\": macro_f1, \"accuracy\": acc}\n","\n","batch_size = 16\n","\n","#Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"task_a_codebert\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    logging_steps=200,\n","    save_steps=2000,\n","    logging_dir=\"logs\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tok,\n","    eval_dataset=val_tok,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n"],"metadata":{"id":"lRhKZBKgFfbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","trainer.train()\n","eval_results = trainer.evaluate(eval_dataset=val_tok)\n","print(\"Validation results:\", eval_results)"],"metadata":{"id":"nyEzG8YxFuSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    confusion_matrix,\n","    classification_report,\n",")\n","\n","#Prediction on validation set\n","val_outputs = trainer.predict(val_tok)\n","val_logits  = val_outputs.predictions            # [N_val, num_labels]\n","val_preds   = np.argmax(val_logits, axis=-1)     # predicted class ids\n","y_val_true = df_val[\"label\"].astype(int).values\n","\n","val_acc = accuracy_score(y_val_true, val_preds)\n","val_f1  = f1_score(y_val_true, val_preds, average=\"macro\")\n","\n","print(f\"Validation Accuracy : {val_acc:.4f}\")\n","print(f\"Validation Macro F1 : {val_f1:.4f}\")\n","\n","#confusion matrix\n","cm = confusion_matrix(y_val_true, val_preds)\n","print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n","print(cm)\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_val_true, val_preds, digits=4))\n","\n","#probabilities (softmax over logits)\n","val_probs = torch.softmax(torch.from_numpy(val_logits), dim=-1).numpy()\n","\n","\n","print(\"\\nFirst 5 (true, pred, prob_0, prob_1):\")\n","for i in range(5):\n","    print(\n","        f\"i={i:3d} | y={y_val_true[i]} | Å·={val_preds[i]} \"\n","        f\"| p0={val_probs[i,0]:.4f} | p1={val_probs[i,1]:.4f}\"\n","    )\n","\n","\n","\n","np.save(\"val_probs_codebert.npy\", val_probs) #saved for ensemble later\n","np.save(\"val_labels.npy\", y_val_true)\n","\n","print(\"\\nSaved for ensemble:- val_probs_codebert.npy and val_labels.npy\")\n","\n","\n","\n"],"metadata":{"id":"_J1qTekIFkLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","save_dir = \"/content/drive/MyDrive/semeval_task13_models/task_a_codebert_v1\"\n","\n","import os\n","os.makedirs(save_dir, exist_ok=True)\n","\n","trainer.save_model(save_dir)          #saves fine-tuned model's weights and configurations\n","tokenizer.save_pretrained(save_dir)   #saves tokenizer\n","\n","print(\"model and tokenizer saved to:\", save_dir)\n"],"metadata":{"id":"yZDQfuQ_Bsq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#prediction on test\n","test_tok.set_format(type=\"torch\")\n","\n","print(\"Running prediction on test set...\")\n","test_outputs = trainer.predict(test_tok)\n","test_logits  = test_outputs.predictions\n","\n","#probablities of each class(for ensemble model)\n","test_probs = torch.softmax(torch.from_numpy(test_logits), dim=-1).numpy()\n","test_preds = np.argmax(test_logits, axis=-1).astype(int)\n","\n","print(\"Test predictions shape:\", test_preds.shape)\n","print(\"Test probs shape      :\", test_probs.shape)\n","print(\"First 10 predictions  :\", test_preds[:10])\n","\n","np.save(\"test_probs_codebert.npy\", test_probs)\n","\n","print(\"\\nSaved for ensemble:test_probs_codebert.npy\")\n","\n","\n","#submission into kaggle using sample submission file\n","sample_filename = \"sample_submission_a.csv\"\n","print(\"Loaded file:\", sample_filename)\n","sample_sub = pd.read_csv(sample_filename)\n","\n","print(\"Sample submission shape:\", sample_sub.shape)\n","print(\"First rows of sample submission:\")\n","print(sample_sub.head())\n","print(\"Number of test predictions:\", len(test_preds))\n","\n","#filling and saving label column\n","if len(sample_sub) != len(test_preds):\n","    print(\"Length mismatch: sample_sub rows:\", len(sample_sub), \"| test_preds:\", len(test_preds))\n","else:\n","    if \"label\" in sample_sub.columns:\n","        label_col = \"label\"\n","    else:\n","        label_col = sample_sub.columns[1]\n","\n","    sample_sub[label_col] = test_preds\n","\n","    print(\"\\nSubmission preview:\")\n","    print(sample_sub.head())\n","\n","    sub_path = \"subtask_a_codebert_final.csv\"\n","    sample_sub.to_csv(sub_path, index=False)\n","    print(f\"\\nSaved submission file: {sub_path}\")\n"],"metadata":{"id":"QVqCMI2bG4Bf"},"execution_count":null,"outputs":[]}]}