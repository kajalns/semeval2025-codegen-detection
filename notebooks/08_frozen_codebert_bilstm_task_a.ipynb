{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN4HxaxzCrzLisWSjVx74+S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HnzuJrqVUAhA"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","\n","from datasets import Dataset\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModel,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding,\n",")\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    confusion_matrix,\n","    classification_report,\n",")\n","\n","#disable wandb\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"WANDB_MODE\"] = \"disabled\"\n"]},{"cell_type":"code","source":["#mount google drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n"],"metadata":{"id":"iKTFuT99Vdzo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#loading finetuned codebert from drive\n","CODEBERT_DIR = \"/content/drive/MyDrive/semeval_task13_models/task_a_codebert_v1\"\n","\n","#load tokenizer & encoder(no classifier head)\n","tokenizer = AutoTokenizer.from_pretrained(CODEBERT_DIR)\n","codebert_encoder = AutoModel.from_pretrained(CODEBERT_DIR)\n","\n","#ensure pad token exists\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token or tokenizer.cls_token\n"],"metadata":{"id":"21xBBY-iVwGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip -q install kaggle\n","\n","import os, shutil\n","from google.colab import files\n","\n","os.makedirs('/root/.kaggle', exist_ok=True)\n","print(\"Upload kaggle.json \")\n","uploaded = files.upload()\n","\n","fname = next(iter(uploaded.keys()))\n","shutil.move(fname, '/root/.kaggle/kaggle.json')\n","os.chmod('/root/.kaggle/kaggle.json', 0o600)\n","\n","!kaggle --version\n","!kaggle datasets list -s \"semeval task 13\" | head -n 10"],"metadata":{"id":"s1yjqp7ZV6P2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SLUG = \"daniilor/semeval-2026-task13\"\n","TARGET = \"/content/semeval_task13\"\n","!mkdir -p \"$TARGET\"\n","!kaggle datasets download -d \"$SLUG\" -p \"$TARGET\"\n","!unzip -o \"$TARGET\"/*.zip -d \"$TARGET\""],"metadata":{"id":"f3ZU0tpAWItZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#step 1:\n","from pathlib import Path\n","\n","import pandas as pd\n","import numpy as np\n","import joblib\n","\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","ROOT = Path(\"/content/semeval_task13\")\n","\n","#handling extra folder layer if present\n","if (ROOT / \"SemEval-2026-Task13\").exists():\n","    BASE_DIR = ROOT / \"SemEval-2026-Task13\"\n","else:\n","    BASE_DIR = ROOT\n","\n","TASK_A_DIR = BASE_DIR / \"task_a\"\n","\n","print(\"BASE_DIR:\", BASE_DIR)\n","print(\"TASK_A_DIR:\", TASK_A_DIR)\n","print(\"Files in task_a:\")\n","for p in TASK_A_DIR.iterdir():\n","    print(\" -\", p.name)\n"],"metadata":{"id":"mstGtGNfWI3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load subtask A data\n","\n","train_path = TASK_A_DIR / \"task_a_training_set_1.parquet\"\n","val_path   = TASK_A_DIR / \"task_a_validation_set.parquet\"\n","test_path  = TASK_A_DIR /\"task_a_test_set_sample.parquet\"\n","\n","df_train = pd.read_parquet(train_path)\n","df_val   = pd.read_parquet(val_path)\n","df_test  = pd.read_parquet(test_path)\n","\n","print(\"Train shape:\", df_train.shape)\n","print(\"Val shape  :\", df_val.shape)\n","print(\"Test shape :\", df_test.shape)\n","print(\"\\nColumns:\", df_train.columns.tolist())\n","\n","df_train.head()\n"],"metadata":{"id":"2_0kRUNNWRtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#basic cleaning\n","df_train = df_train[[\"code\", \"label\"]].dropna()\n","df_val   = df_val[[\"code\", \"label\"]].dropna()\n","df_test  = df_test[[\"code\"]].copy()  # test has no labels\n","\n","df_train[\"label\"] = df_train[\"label\"].astype(int)\n","df_val[\"label\"]   = df_val[\"label\"].astype(int)\n","\n","print(\"Train shape:\", df_train.shape)\n","print(\"Val shape  :\", df_val.shape)\n","print(\"Test shape :\", df_test.shape)\n"],"metadata":{"id":"xvpdsyd0WJX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert to HF dataset and tokenize\n","train_ds = Dataset.from_pandas(df_train, preserve_index=False)\n","val_ds   = Dataset.from_pandas(df_val,   preserve_index=False)\n","test_ds  = Dataset.from_pandas(df_test,  preserve_index=False)\n","\n","train_ds = train_ds.rename_column(\"label\", \"labels\")\n","val_ds   = val_ds.rename_column(\"label\", \"labels\")\n","\n","max_length = 256\n","\n","def tokenize_fn(batch):\n","    return tokenizer(\n","        batch[\"code\"],\n","        truncation=True,\n","        max_length=max_length,\n","    )\n","\n","train_tok = train_ds.map(tokenize_fn, batched=True)\n","val_tok   = val_ds.map(tokenize_fn,   batched=True)\n","test_tok  = test_ds.map(tokenize_fn,  batched=True)\n","\n","#remove raw text column\n","train_tok = train_tok.remove_columns([\"code\"])\n","val_tok   = val_tok.remove_columns([\"code\"])\n","test_tok  = test_tok.remove_columns([\"code\"])\n","\n","#set format for PyTorch\n","train_tok.set_format(type=\"torch\")\n","val_tok.set_format(type=\"torch\")\n","test_tok.set_format(type=\"torch\")\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","print(train_tok)\n","print(val_tok)\n","print(test_tok)\n"],"metadata":{"id":"dVSTb-8pXZf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hybrid model(codebert+BiLSTM+linear layer)\n","class CodeBertBiLSTMClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        encoder,\n","        num_labels=2,\n","        lstm_hidden_size=256,\n","        lstm_num_layers=1,\n","        bidirectional=True,\n","        dropout=0.1,\n","        freeze_encoder=True,\n","    ):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.num_labels = num_labels\n","\n","        hidden_size = encoder.config.hidden_size  # 768 for base CodeBERT\n","\n","        #optionally freeze encoder to train only BiLSTM + classifier\n","        if freeze_encoder:\n","            for param in self.encoder.parameters():\n","                param.requires_grad = False\n","\n","        self.lstm = nn.LSTM(\n","            input_size=hidden_size,\n","            hidden_size=lstm_hidden_size,\n","            num_layers=lstm_num_layers,\n","            batch_first=True,\n","            bidirectional=bidirectional,\n","        )\n","\n","        lstm_out_dim = lstm_hidden_size * (2 if bidirectional else 1)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.classifier = nn.Linear(lstm_out_dim, num_labels)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        labels=None,\n","        **kwargs,\n","    ):\n","        #encoder outputs(sequence of hidden states)\n","        encoder_outputs = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","        )\n","        #[batch, seq_len, hidden]\n","        last_hidden_state = encoder_outputs.last_hidden_state\n","\n","        #BiLSTM over token sequence\n","        lstm_out, _ = self.lstm(last_hidden_state)  # batch, seq_len, lstm_out_dim]\n","\n","        #use last time step\n","        last_hidden = lstm_out[:, -1, :]            #[batch, lstm_out_dim]\n","\n","        logits = self.classifier(self.dropout(last_hidden))\n","\n","        loss = None\n","        if labels is not None:\n","            loss = self.loss_fn(logits, labels)\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=None,\n","            attentions=None,\n","        )\n","\n","#instantiate hybrid model(encoder frozen)\n","hybrid_model = CodeBertBiLSTMClassifier(\n","    encoder=codebert_encoder,\n","    num_labels=2,\n","    lstm_hidden_size=256,\n","    lstm_num_layers=1,\n","    bidirectional=True,\n","    dropout=0.1,\n","    freeze_encoder=True,\n",")\n"],"metadata":{"id":"gU_ZJjReXmAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#metric function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","\n","    macro_f1 = f1_score(labels, preds, average=\"macro\")\n","    acc      = accuracy_score(labels, preds)\n","\n","    return {\"macro_f1\": macro_f1, \"accuracy\": acc}\n"],"metadata":{"id":"Qv8iEvAAXu-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TrainingArguments & Trainer\n","\n","batch_size = 16\n","\n","training_args = TrainingArguments(\n","    output_dir=\"task_a_codebert_bilstm\",   #temporary folder\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=2,\n","    learning_rate=2e-4,\n","    weight_decay=0.01,\n","    logging_steps=200,\n","    save_strategy=\"no\",\n","    logging_dir=\"logs_codebert_bilstm\",\n","    report_to=[],\n",")\n","\n","trainer_hybrid = Trainer(\n","    model=hybrid_model,\n","    args=training_args,\n","    train_dataset=train_tok,\n","    eval_dataset=val_tok,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n"],"metadata":{"id":"PfDdgtMtXw5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#training and evaluation of hybrid model\n","trainer_hybrid.train()\n","\n","eval_results = trainer_hybrid.evaluate(eval_dataset=val_tok)\n","print(\"Hybrid Validation results:\", eval_results)\n"],"metadata":{"id":"3jzJ02jrX9Yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#detailed validation metrics\n","hyb_val_outputs = trainer_hybrid.predict(val_tok)\n","hyb_val_logits  = hyb_val_outputs.predictions\n","hyb_val_preds   = hyb_val_logits.argmax(axis=-1)\n","\n","y_val_true = df_val[\"label\"].astype(int).values\n","\n","val_acc = accuracy_score(y_val_true, hyb_val_preds)\n","val_f1  = f1_score(y_val_true, hyb_val_preds, average=\"macro\")\n","\n","print(f\"\\nHybrid Validation Accuracy : {val_acc:.4f}\")\n","print(f\"Hybrid Validation Macro F1 : {val_f1:.4f}\")\n","\n","cm = confusion_matrix(y_val_true, hyb_val_preds)\n","print(\"\\nHybrid Confusion Matrix (rows=true, cols=pred):\")\n","print(cm)\n","\n","print(\"\\nHybrid Classification Report:\")\n","print(classification_report(y_val_true, hyb_val_preds, digits=4))\n"],"metadata":{"id":"32lvgoVuYEHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#prediction on test set\n","test_outputs = trainer_hybrid.predict(test_tok)\n","test_logits  = test_outputs.predictions\n","test_preds   = test_logits.argmax(axis=-1).astype(int)\n","\n","print(\"Hybrid test preds shape:\", test_preds.shape)\n","print(\"First 10 hybrid preds:\", test_preds[:10])\n"],"metadata":{"id":"lNP3NYunXZls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_filename = \"sample_submission_a.csv\"\n","sample_sub = pd.read_csv(sample_filename)\n","\n","print(\"Sample submission shape:\", sample_sub.shape)\n","print(\"Number of test predictions:\", len(test_preds))\n","\n","if len(sample_sub) != len(test_preds):\n","    print(\"length mismatch: sample_sub rows:\", len(sample_sub), \"| test_preds:\", len(test_preds))\n","else:\n","    if \"label\" in sample_sub.columns:\n","        label_col = \"label\"\n","    else:\n","        label_col = sample_sub.columns[1]\n","\n","    sample_sub[label_col] = test_preds\n","\n","    sub_path = \"subtask_a_codebert_bilstm.csv\"\n","    sample_sub.to_csv(sub_path, index=False)\n","    print(f\"\\nsaved hybrid submission file: {sub_path}\")\n","    print(sample_sub.head())\n"],"metadata":{"id":"mz0GhDLwYe1u"},"execution_count":null,"outputs":[]}]}